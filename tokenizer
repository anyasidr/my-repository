"""This module is used for tokenizing strings.
The string must be divided into alphabetic characters.""" 
import re


class Tokenizer(object):
    def tokenize(self, text):
        tokens = []      
        s = re.sub(r'[\W\d_]', ' ', text)#replacing non-alphabetic characters except hyphen with spaces
        pattern = re.compile("[^\W]*[^\W]")#searching for compound words as well
        for match in re.finditer(pattern, s): #here our text is divided into words from space to space 
            begin = match.start()
            m = (match.group(), begin)
            for token in m:
                tokens.append(token)
        return tokens


text = "доброе утро44 !!! - ++ 6&13 **(   спокойной темно-синий  441 ночи привет. Стол - это предмет мебели"
words = Tokenizer().tokenize(text)
print(words)
            
